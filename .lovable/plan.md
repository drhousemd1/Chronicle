

# Fix Base64 Image Storage Throughout the Application

## The Problem

Images generated by the AI (covers and avatars) are sometimes stored as raw base64 strings directly in database columns instead of being uploaded to storage buckets. A single base64 image is 2-2.5MB of text. When the gallery loads 3 published scenarios and one has a base64 cover, the database response is ~2.5MB of JSON -- causing the 15-20 second load time.

This is NOT isolated to the gallery. It affects every page that loads images.

### Current Data State
- **1 cover image** stored as base64 (2.5MB) out of 4 total scenarios
- **11 character avatars** stored as base64 (~2MB each) out of 27 total characters
- The remaining images correctly use storage bucket URLs (~170 characters each)

### Pages Affected
- Community Gallery (loads all published covers -- worst case)
- Your Stories hub (loads all your scenario covers)
- Conversation list sidebar (loads scenario thumbnails)
- Creator Profile and Public Profile pages (loads published works with covers)
- Any page that loads a character avatar

## Root Cause

The `generate-cover-image` edge function receives images from the xAI API. When the API returns a `b64_json` response (instead of a URL), the function wraps it as `data:image/png;base64,...` and returns it to the frontend. The frontend then saves that raw 2.5MB string directly into the `cover_image_url` database column -- instead of uploading the image to the `covers` storage bucket and saving the short URL.

The same issue exists for avatar generation edge functions.

## The Fix (3 parts)

### Part 1: Fix the edge functions (prevent future base64 storage)

Update `generate-cover-image`, `generate-side-character-avatar`, and any other image generation edge functions to:
1. When receiving `b64_json` from the API, convert it to a Blob
2. Upload the blob to the appropriate storage bucket (`covers` or `avatars`)
3. Return the public storage URL instead of the base64 string

This ensures all NEW images go through storage.

### Part 2: Migrate existing base64 images to storage

Write a one-time migration edge function that:
1. Scans `scenarios.cover_image_url` and `characters.avatar_url` for values starting with `data:`
2. Converts each base64 string to a file
3. Uploads it to the correct storage bucket
4. Updates the database row with the new storage URL

This cleans up all existing bad data.

### Part 3: Add a safety net in the frontend

In `src/services/supabase-data.ts`, add a utility function that intercepts base64 data URLs before saving. If a `cover_image_url` or `avatar_url` value starts with `data:`, upload it to storage first and use the resulting URL. This acts as a safety net in case any code path still passes base64 data.

## Visual Impact

**None.** The images will look exactly the same. The only difference is where the image bytes live -- in a storage bucket (fast, efficient) instead of inline in a database column (slow, bloated). The storage bucket URLs load just as fast as any normal image URL.

## Technical Details

### Edge Function Changes (`generate-cover-image/index.ts`)

Replace the base64 fallback (lines 99-103):

```typescript
// BEFORE (bad):
if (data.data?.[0]?.b64_json) {
  imageUrl = `data:image/png;base64,${data.data[0].b64_json}`;
}

// AFTER (good):
if (data.data?.[0]?.b64_json) {
  // Upload to storage instead of returning base64
  const imageBytes = Uint8Array.from(atob(data.data[0].b64_json), c => c.charCodeAt(0));
  const filename = `${user.id}/cover-${Date.now()}.png`;
  const { error: uploadError } = await supabase.storage.from('covers').upload(filename, imageBytes, { contentType: 'image/png', upsert: true });
  if (uploadError) throw uploadError;
  const { data: urlData } = supabase.storage.from('covers').getPublicUrl(filename);
  imageUrl = urlData.publicUrl;
}
```

Same pattern for `generate-side-character-avatar` and similar functions.

### Migration Function

A new edge function `migrate-base64-images` that:
1. Uses the service role key to read all rows with `data:` prefixed URLs
2. Converts and uploads each to storage
3. Updates the rows with new URLs
4. Returns a summary of what was migrated

### Frontend Safety Net (`src/services/supabase-data.ts`)

Add a helper used in `saveScenario` and character save functions:

```typescript
async function ensureStorageUrl(dataUrl: string, bucket: string, userId: string): Promise<string> {
  if (!dataUrl.startsWith('data:')) return dataUrl; // Already a URL
  const blob = dataUrlToBlob(dataUrl);
  if (!blob) return dataUrl;
  const filename = `${userId}/${bucket}-${Date.now()}.png`;
  const { error } = await supabase.storage.from(bucket).upload(filename, blob, { upsert: true });
  if (error) throw error;
  const { data } = supabase.storage.from(bucket).getPublicUrl(filename);
  return data.publicUrl;
}
```

### GalleryHub.tsx Duplicate Fetch Prevention

Add a `useRef` guard to prevent multiple concurrent fetches caused by `user` object identity changes:

```typescript
const fetchInProgress = useRef(false);
const loadScenarios = useCallback(async () => {
  if (fetchInProgress.current) return;
  fetchInProgress.current = true;
  // ... existing logic ...
  fetchInProgress.current = false;
}, [user?.id, searchTags, sortBy, getContentThemeFilters]);
// Changed: user -> user?.id
```

## Order of Operations

1. Fix edge functions (prevent new base64 storage)
2. Add frontend safety net (catch any remaining paths)
3. Add duplicate fetch guard in GalleryHub
4. Run migration to clean up existing base64 data
